{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from statistics import mean\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent\n",
    "threshold = 100\n",
    "num_iter = 20\n",
    "learning_rate = 0.01\n",
    "reg_factor = 10\n",
    "\n",
    "\"\"\"\n",
    "Read data from the specified training, validation and test data files.\n",
    "\"\"\"\n",
    "def read_data(trainFile, valFile, testFile):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    # read training, test, and validation data\n",
    "    for file in [trainFile, valFile, testFile]:\n",
    "        # read data\n",
    "        data = np.loadtxt(file)\n",
    "\n",
    "        # transform into our feature space with \"fe()\"\n",
    "        features.append(fe(torch.tensor(data[:,:-1])))\n",
    "\n",
    "        labels.append(torch.tensor(data[:,-1]))\n",
    "    \n",
    "    return features[0], labels[0], features[1], \\\n",
    "        labels[1], features[2], labels[2]\n",
    "\n",
    "\"\"\"\n",
    "Feature that counts the number of pixels above a specified threshold\n",
    "in each row and column.\n",
    "\"\"\"\n",
    "def fe(X):\n",
    "    # get a \"binary image\" indicator of pixels above and below the threshold\n",
    "    X_binary = torch.where(X > threshold,\n",
    "        torch.ones_like(X),torch.zeros_like(X)).reshape(-1,28,28)\n",
    "    \n",
    "    # calculate row and column features\n",
    "    X_row = X_binary.sum(dim=1)\n",
    "    X_col = X_binary.sum(dim=2)\n",
    "\n",
    "    # include a row of ones at the end for bias\n",
    "    return torch.cat([X_row, X_col, torch.ones(X_row.shape[0], \n",
    "        1, dtype=torch.float64)], dim=1)\n",
    "\n",
    "\"\"\"\n",
    "Convert \"y\" into its one-hot-encoding equivalent.\n",
    "\"\"\"\n",
    "def one_hot(y):\n",
    "    y_one_hot = torch.zeros([y.shape[0], 10], dtype=torch.float64)\n",
    "    return y_one_hot.scatter(1, y.reshape(-1, 1).to(torch.long), 1)\n",
    "\n",
    "\"\"\"\n",
    "Get the scores for each class for each training point.\n",
    "\"\"\"\n",
    "def get_scores(X, w):\n",
    "    scores = torch.mm(X, w.T)\n",
    "    \n",
    "    return scores\n",
    "    \n",
    "\"\"\"\n",
    "Train the model using regularized logistic regression.\n",
    "\"\"\"\n",
    "def train(X,y):\n",
    "    # convert index labels of y into a one-hot encoding\n",
    "    one_hot_y = one_hot(y)\n",
    "\n",
    "    # loss list over iterations for plotting\n",
    "    losses = []\n",
    "\n",
    "    # initialize model weights\n",
    "    w = torch.rand((10, X.shape[1]), dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "    i = 0\n",
    "    while i < num_iter:\n",
    "        # Calculate the loss function, using stochastic \n",
    "        # gradient descent if applicable\n",
    "        \n",
    "        m = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        reg_term = reg_factor * torch.sum(w**2)\n",
    "        \n",
    "        inp = torch.mm(X, w.T)\n",
    "        \n",
    "        target = y.type(torch.long)\n",
    "        \n",
    "        loss = m(inp, target) + reg_term\n",
    "\n",
    "        # calculate loss gradient \n",
    "        loss.backward()\n",
    "\n",
    "        # save for plotting\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            w.sub_(learning_rate * w.grad)\n",
    "        w.grad.data.zero_()\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    return w, losses\n",
    "\n",
    "\"\"\"\n",
    "Get list of predicted labels for feature set \"X\" using model \n",
    "parameterized by w.\n",
    "\"\"\"\n",
    "def predict(X, w):\n",
    "    # get scores for each class for each input\n",
    "    scores = get_scores(X, w)\n",
    "\n",
    "    # find the index of the maximum score for each input,\n",
    "    # which happens to exactly correspond to the label!\n",
    "    return torch.argmax(scores, dim=1)\n",
    "\n",
    "\"\"\"\n",
    "Evaluate the model parameterized by \"w\", using unseen data features \"X\" and\n",
    "corresponding labels \"y\".\n",
    "\"\"\"\n",
    "def evaluate(X, y, w):\n",
    "    # use model to get predictions\n",
    "    predictions = predict(X, w)\n",
    "    \n",
    "    # total number of items in dataset\n",
    "    total = y.shape[0]\n",
    "\n",
    "    # number of correctly labeled items in dataset\n",
    "    correct = torch.sum(predictions == y.long())\n",
    "\n",
    "    # return fraction of correctly labeled items in dataset\n",
    "    return float(correct) / float(total)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load data from file\n",
    "    train_vec, train_lab, val_vec, val_lab, test_vec, test_lab \\\n",
    "        = read_data('hw0train.txt','hw0validate.txt', 'hw0test.txt')\n",
    "    \n",
    "\n",
    "    # find w through gradient descent\n",
    "    w, losses = train(train_vec,train_lab)\n",
    "\n",
    "    # evaluate model on validation data\n",
    "    accuracy = evaluate(val_vec, val_lab, w)\n",
    " \n",
    "    print(\"Validation accuracy: {}\".format(accuracy))\n",
    "\n",
    "    # plot losses\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss plot\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stochastic Gradient Descent\n",
    "threshold = 100\n",
    "learning_rate = 0.0001\n",
    "reg_factor = 1\n",
    "epoch = 10\n",
    "batch_size = 1\n",
    "\n",
    "\"\"\"\n",
    "Read data from the specified training, validation and test data files.\n",
    "\"\"\"\n",
    "def read_data(trainFile, valFile, testFile):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    # read training, test, and validation data\n",
    "    for file in [trainFile, valFile, testFile]:\n",
    "        # read data\n",
    "        data = np.loadtxt(file)\n",
    "\n",
    "        # transform into our feature space with \"fe()\"\n",
    "        features.append(fe(torch.tensor(data[:,:-1])))\n",
    "\n",
    "        labels.append(torch.tensor(data[:,-1]))\n",
    "    \n",
    "    return features[0], labels[0], features[1], \\\n",
    "        labels[1], features[2], labels[2]\n",
    "\n",
    "\"\"\"\n",
    "Feature that counts the number of pixels above a specified threshold\n",
    "in each row and column.\n",
    "\"\"\"\n",
    "def fe(X):\n",
    "    # get a \"binary image\" indicator of pixels above and below the threshold\n",
    "    X_binary = torch.where(X > threshold,\n",
    "        torch.ones_like(X),torch.zeros_like(X)).reshape(-1,28,28)\n",
    "    \n",
    "    # calculate row and column features\n",
    "    X_row = X_binary.sum(dim=1)\n",
    "    X_col = X_binary.sum(dim=2)\n",
    "\n",
    "    # include a row of ones at the end for bias\n",
    "    return torch.cat([X_row, X_col, torch.ones(X_row.shape[0], \n",
    "        1, dtype=torch.float64)], dim=1)\n",
    "\n",
    "\"\"\"\n",
    "Convert \"y\" into its one-hot-encoding equivalent.\n",
    "\"\"\"\n",
    "def one_hot(y):\n",
    "    y_one_hot = torch.zeros([y.shape[0], 10], dtype=torch.float64)\n",
    "    return y_one_hot.scatter(1, y.reshape(-1, 1).to(torch.long), 1)\n",
    "\n",
    "\"\"\"\n",
    "Get the scores for each class for each training point.\n",
    "\"\"\"\n",
    "def get_scores(X, w):\n",
    "    # Calculate score matrix (should be N x 10)\n",
    "\n",
    "    scores = torch.mm(X, w.T)\n",
    "    \n",
    "    return scores\n",
    "    \n",
    "\"\"\"\n",
    "Train the model using regularized logistic regression.\n",
    "\"\"\"\n",
    "def train(X,y):\n",
    "    X, y = shuffle(X, y)\n",
    "    \n",
    "    # convert index labels of y into a one-hot encoding\n",
    "    one_hot_y = one_hot(y)\n",
    "\n",
    "    # loss list over iterations for plotting\n",
    "    losses = []\n",
    "\n",
    "    # initialize model weights\n",
    "    w = torch.rand((10, X.shape[1]), dtype=torch.float64, requires_grad=True)\n",
    "    \n",
    "    n_iter = len(X) // batch_size\n",
    "\n",
    "    for _ in range(epoch):\n",
    "        for i in range(n_iter):\n",
    "        \n",
    "            m = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "            reg_term = reg_factor * torch.sum(w**2)\n",
    "\n",
    "            inp = torch.mm(X[i*batch_size : (i+1) * batch_size], w.T)\n",
    "\n",
    "            target = y[i*batch_size : (i+1) * batch_size].type(torch.long)\n",
    "\n",
    "            loss = m(inp, target) + reg_term\n",
    "\n",
    "            # calculate loss gradient \n",
    "            loss.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                w.sub_(learning_rate * w.grad)\n",
    "            w.grad.data.zero_()\n",
    "        \n",
    "        # save for plotting\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return w, losses\n",
    "\n",
    "\"\"\"\n",
    "Get list of predicted labels for feature set \"X\" using model \n",
    "parameterized by w.\n",
    "\"\"\"\n",
    "def predict(X, w):\n",
    "    # get scores for each class for each input\n",
    "    scores = get_scores(X, w)\n",
    "    \n",
    "\n",
    "    # find the index of the maximum score for each input,\n",
    "    # which happens to exactly correspond to the label!\n",
    "    return torch.argmax(scores, dim=1)\n",
    "\n",
    "\"\"\"\n",
    "Evaluate the model parameterized by \"w\", using unseen data features \"X\" and\n",
    "corresponding labels \"y\".\n",
    "\"\"\"\n",
    "def evaluate(X, y, w):\n",
    "    # use model to get predictions\n",
    "    predictions = predict(X, w)\n",
    "    \n",
    "    # total number of items in dataset\n",
    "    total = y.shape[0]\n",
    "\n",
    "    # number of correctly labeled items in dataset\n",
    "    correct = torch.sum(predictions == y.long())\n",
    "\n",
    "    # return fraction of correctly labeled items in dataset\n",
    "    return float(correct) / float(total)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load data from file\n",
    "    train_vec, train_lab, val_vec, val_lab, test_vec, test_lab \\\n",
    "        = read_data('hw0train.txt','hw0validate.txt', 'hw0test.txt')\n",
    "\n",
    "    # find w through gradient descent\n",
    "    w, losses = train(train_vec,train_lab)\n",
    "\n",
    "    # evaluate model on validation data\n",
    "    accuracy = evaluate(val_vec, val_lab, w)\n",
    "        \n",
    "    print(\"Validation accuracy: {}\".format(accuracy))\n",
    "\n",
    "    # plot losses\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss plot\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimized Stochastic Gradient Descent\n",
    "threshold = 100\n",
    "learning_rate = 0.0001\n",
    "reg_factor = 1\n",
    "epoch = 10\n",
    "batch_size = 1\n",
    "\n",
    "\"\"\"\n",
    "Read data from the specified training, validation and test data files.\n",
    "\"\"\"\n",
    "def read_data(trainFile, valFile, testFile):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    # read training, test, and validation data\n",
    "    for file in [trainFile, valFile, testFile]:\n",
    "        # read data\n",
    "        data = np.loadtxt(file)\n",
    "\n",
    "        # transform into our feature space with \"fe()\"\n",
    "        features.append(fe(torch.tensor(data[:,:-1])))\n",
    "\n",
    "        labels.append(torch.tensor(data[:,-1]))\n",
    "    \n",
    "    return features[0], labels[0], features[1], \\\n",
    "        labels[1], features[2], labels[2]\n",
    "\n",
    "\"\"\"\n",
    "Feature that counts the number of pixels above a specified threshold\n",
    "in each row and column.\n",
    "\"\"\"\n",
    "def fe(X):\n",
    "    # get a \"binary image\" indicator of pixels above and below the threshold\n",
    "    X_binary = torch.where(X > threshold,\n",
    "        torch.ones_like(X),torch.zeros_like(X)).reshape(-1,28,28)\n",
    "    \n",
    "    # calculate row and column features\n",
    "    X_row = X_binary.sum(dim=1)\n",
    "    X_col = X_binary.sum(dim=2)\n",
    "\n",
    "    # include a row of ones at the end for bias\n",
    "    return torch.cat([X_row, X_col, torch.ones(X_row.shape[0], \n",
    "        1, dtype=torch.float64)], dim=1)\n",
    "\n",
    "\"\"\"\n",
    "Convert \"y\" into its one-hot-encoding equivalent.\n",
    "\"\"\"\n",
    "def one_hot(y):\n",
    "    y_one_hot = torch.zeros([y.shape[0], 10], dtype=torch.float64)\n",
    "    return y_one_hot.scatter(1, y.reshape(-1, 1).to(torch.long), 1)\n",
    "\n",
    "\"\"\"\n",
    "Get the scores for each class for each training point.\n",
    "\"\"\"\n",
    "def get_scores(X, w):\n",
    "    # Calculate score matrix (should be N x 10)\n",
    "\n",
    "    scores = torch.mm(X, w.T)\n",
    "    \n",
    "    return scores\n",
    "    \n",
    "\"\"\"\n",
    "Train the model using regularized logistic regression.\n",
    "\"\"\"\n",
    "def train(X,y):\n",
    "    X, y = shuffle(X, y)\n",
    "    \n",
    "    # convert index labels of y into a one-hot encoding\n",
    "    one_hot_y = one_hot(y)\n",
    "\n",
    "    # loss list over iterations for plotting\n",
    "    losses = []\n",
    "\n",
    "    # initialize model weights\n",
    "    w = torch.rand((10, X.shape[1]), dtype=torch.float64, requires_grad=True)\n",
    "    \n",
    "    n_iter = len(X) // batch_size\n",
    "    \n",
    "    optimizer = torch.optim.SGD([w], learning_rate)\n",
    "\n",
    "    for _ in range(epoch):\n",
    "        for i in range(n_iter):\n",
    "        \n",
    "            m = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "            reg_term = reg_factor * torch.sum(w**2)\n",
    "\n",
    "            inp = torch.mm(X[i*batch_size : (i+1) * batch_size], w.T)\n",
    "\n",
    "            target = y[i*batch_size : (i+1) * batch_size].type(torch.long)\n",
    "\n",
    "            loss = m(inp, target) + reg_term\n",
    "\n",
    "            # calculate loss gradient \n",
    "            loss.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # save for plotting\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return w, losses\n",
    "\n",
    "\"\"\"\n",
    "Get list of predicted labels for feature set \"X\" using model \n",
    "parameterized by w.\n",
    "\"\"\"\n",
    "def predict(X, w):\n",
    "    # get scores for each class for each input\n",
    "    scores = get_scores(X, w)\n",
    "    \n",
    "\n",
    "    # find the index of the maximum score for each input,\n",
    "    # which happens to exactly correspond to the label!\n",
    "    return torch.argmax(scores, dim=1)\n",
    "\n",
    "\"\"\"\n",
    "Evaluate the model parameterized by \"w\", using unseen data features \"X\" and\n",
    "corresponding labels \"y\".\n",
    "\"\"\"\n",
    "def evaluate(X, y, w):\n",
    "    # use model to get predictions\n",
    "    predictions = predict(X, w)\n",
    "    \n",
    "    # total number of items in dataset\n",
    "    total = y.shape[0]\n",
    "\n",
    "    # number of correctly labeled items in dataset\n",
    "    correct = torch.sum(predictions == y.long())\n",
    "\n",
    "    # return fraction of correctly labeled items in dataset\n",
    "    return float(correct) / float(total)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load data from file\n",
    "    train_vec, train_lab, val_vec, val_lab, test_vec, test_lab \\\n",
    "        = read_data('hw0train.txt','hw0validate.txt', 'hw0test.txt')\n",
    "\n",
    "    # find w through gradient descent\n",
    "    w, losses = train(train_vec,train_lab)\n",
    "\n",
    "    # evaluate model on validation data\n",
    "    accuracy = evaluate(val_vec, val_lab, w)\n",
    "        \n",
    "    print(\"Validation accuracy: {}\".format(accuracy))\n",
    "\n",
    "    # plot losses\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss plot\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
